# AI Financial Analyst Dashboard

An interactive web application that uses a combination of machine learning, news sentiment analysis, and AI to provide predictive insights into the stock market.

This dashboard allows users to select a major stock and receive a detailed analysis, including a prediction of the next hour's price movement and a summary report generated by a Large Language Model (LLM).

## Table of Contents
- [Introduction](#introduction)
- [Live Demo](#live-demo)
- [Features](#features)
- [Tech Stack & Architecture](#tech-stack--architecture)
- [The Workflow: A Deep Dive](#the-workflow-a-deep-dive)
- [How to Set Up and Run Locally](#how-to-set-up-and-run-locally)

## Introduction

In today's fast-paced financial markets, gaining a predictive edge requires analyzing vast amounts of data, from historical prices to the constant stream of news. This project was built to automate this complex process.

This dashboard is an end-to-end data science application that:

1.  Fetches real-time and historical financial data from live APIs.
2.  Analyzes the sentiment of thousands of news articles to understand market mood.
3.  Trains a sophisticated machine learning model to predict short-term stock price movements.
4.  Presents these findings in an interactive, easy-to-use web interface.
5.  Uses a generative AI to create a human-like summary of the analysis.

## Live Demo

> You can view a live demo of the dashboard here: **[https://stock-prediction-dashboard-shreevatsa-agnihotri.streamlit.app/](https://stock-prediction-dashboard-shreevatsa-agnihotri.streamlit.app/)**

## Features

- **Interactive Dashboard:** A clean, user-friendly interface built with Streamlit.
- **Multi-Stock Analysis:** Select from a dropdown of 25+ major stocks, including top tech and financial companies.
- **Predictive Modeling:** See the model's prediction accuracy and a chart showing its predictions (Up or Down) against the actual price history.
- **AI-Generated Summaries:** Click a button to get a concise, AI-written summary of the quantitative and news sentiment analysis.
- **Technical Deep Dive:** A separate page that provides a detailed log of the entire backend process, including data shapes, model parameters, and visualizations for a technical audience.

## Tech Stack & Architecture

This project uses a modern, end-to-end data science stack, all with free-tier or open-source tools.

- **Frontend:** **Streamlit** (for the interactive web dashboard).
- **Data Sourcing:**
    - **yfinance API:** For historical hourly stock market data (price, volume).
    - **Finnhub API:** For historical company news headlines.
    - **BeautifulSoup:** For scraping recent news headlines for the AI summary.
- **Machine Learning & Data Science:**
    - **Pandas:** For data manipulation and analysis.
    - **Scikit-learn:** For data splitting (`TimeSeriesSplit`) and model tuning (`GridSearchCV`).
    - **XGBoost:** The core machine learning algorithm for prediction.
- **Artificial Intelligence:**
    - **Hugging Face Transformers:** For Natural Language Processing (NLP) to perform sentiment analysis on news headlines.
    - **Groq API (Llama 3):** The Large Language Model (LLM) used to generate the final summary reports.
- **Visualization:**
    - **Plotly:** For interactive, dynamic charts (zoom, pan).
    - **Matplotlib & Seaborn:** For static charts like the feature importance and correlation matrix.

## The Workflow: A Deep Dive

What happens when you select a stock? Here‚Äôs a simple, step-by-step look at the backend process.

### Step 1: Data Collection üìà

The application first gathers two types of data from the internet:

1.  **Market Data:** It uses the `yfinance` API to download the last 2 years of **hourly** price and volume data for the selected stock. This provides thousands of data points to analyze.
2.  **News Data:** It then uses the `Finnhub` API to collect all the news headlines related to that company over the same two-year period.

### Step 2: Feature Engineering & Sentiment Analysis üß†

Raw data isn't very useful to a machine. The application then processes this data to create meaningful "features" or signals.

- **Technical Indicators:** It calculates financial metrics like **Bollinger Bands** and **RSI**, which help the model understand price momentum and volatility.
- **News Sentiment:** This is where Natural Language Processing (NLP) comes in. The application reads every single news headline and uses a **Hugging Face Transformer** model to assign a sentiment score (positive or negative). It then calculates an average sentiment score for each day.

### Step 3: Model Training with XGBoost ‚öôÔ∏è

This is the core predictive step.

- **Algorithm Choice (XGBoost):** We use the **XGBoost** algorithm because it is an industry-leading model known for its high accuracy and performance on structured data, like our financial features.
- **Training:** The model is trained on the historical data to learn the complex relationships between the technical indicators, news sentiment, and the stock's future price movement (whether it went up or down in the next hour).
- **Smart Tuning (GridSearchCV):** To get the best possible performance, the application uses `GridSearchCV` to automatically test dozens of different settings for the XGBoost model and selects the combination that produces the highest accuracy.

### Step 4: Final AI Summary ‚úçÔ∏è

After the model is trained, the user can click a button to get a final report.

- The application sends the model's accuracy and a summary of recent news sentiment to a powerful **Large Language Model (Llama 3 via the Groq API)**.
- This LLM then writes a concise, easy-to-read summary, just like a human analyst would.

## How to Set Up and Run Locally

To run this project on your own computer, please follow these steps.

### 1. Clone the Repository

```bash
git clone [https://github.com/Shreevatsa123/stock-prediction-dashboard.git](https://github.com/Shreevatsa123/stock-prediction-dashboard.git)
cd stock-prediction-dashboard
```

### 2\. Create and Activate a Virtual Environment

It's a best practice to keep project dependencies isolated.

```bash
# Create the virtual environment
python -m venv venv_stck

# Activate it (on Windows)
.\venv_stck\Scripts\activate

# Activate it (on macOS/Linux)
source venv_stck/bin/activate
```

### 3\. Install Dependencies

This command will install all the necessary Python libraries.

```bash
pip install -r requirements.txt
```

### 4\. Add Your API Keys

You will need to get free API keys from [Finnhub](https://finnhub.io) and [Groq](https://groq.com/).

1.  Create a folder in the main project directory named `.streamlit`.
2.  Inside that folder, create a file named `secrets.toml`.
3.  Add your keys to the file like this:
    ```toml
    GROQ_API_KEY = "your-groq-api-key"
    FINNHUB_API_KEY = "your-finnhub-api-key"
    ```

### 5\. Run the Streamlit App

You're all set\! Run the following command in your terminal.

```bash
streamlit run Home.py
```

The application should automatically open in your web browser.
